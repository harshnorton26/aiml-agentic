<!DOCTYPE html>
<html>
<head>
    <title>HR Policy Assistant - Architecture Diagrams</title>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <style>
        body { 
            font-family: Arial, sans-serif; 
            margin: 20px;
            background: #f5f5f5;
        }
        .diagram-container {
            background: white;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 { color: #2196f3; }
        h2 { color: #4caf50; margin-top: 30px; }
        .mermaid { 
            text-align: center;
            background: white;
        }
    </style>
</head>
<body>
    <h1>üèóÔ∏è HR Policy Assistant - Architecture Diagrams</h1>
    
    <div class="diagram-container">
        <h2>1. System Architecture Flowchart</h2>
        <div class="mermaid">
flowchart TD
    User([User/Employee]) -->|Asks Question| UI[Streamlit UI<br/>frontend/app.py]
    UI -->|Initialize Agent| AgentInit{Agent<br/>Ready?}
    AgentInit -->|No| LoadAgent[Load HR Agent<br/>backend/hr_agent.py]
    AgentInit -->|Yes| ProcessQuery[Process User Query]
    LoadAgent --> ProcessQuery
    ProcessQuery --> Agent[HRPolicyAgent<br/>backend/hr_agent.py]
    Agent --> Memory[Simple Memory<br/>Conversation Buffer]
    Agent --> Retriever[PolicyRetriever<br/>backend/tools.py]
    Retriever -->|Keyword Search| PolicyFiles[(Policy Documents<br/>data/policies/*.txt)]
    PolicyFiles -->|Return Relevant Text| Retriever
    Retriever -->|Policy Context| Agent
    Agent -->|Build Prompt| PromptBuilder[Combine:<br/>‚Ä¢ System Prompt<br/>‚Ä¢ Policy Context<br/>‚Ä¢ User Query<br/>‚Ä¢ Chat History]
    PromptBuilder --> SystemPrompt[AGENT_SYSTEM_PROMPT<br/>backend/prompts.py]
    PromptBuilder --> LLM[Ollama/Mistral<br/>localhost:11434]
    LLM -->|Generated Response| Agent
    Agent -->|Store in Memory| Memory
    Agent -->|Return Response| UI
    UI -->|Display Response| User
    Config[config.yaml<br/>config/] -.->|Settings| Agent
    Config -.->|Settings| UI
    classDef frontend fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef backend fill:#f5f5f5,stroke:#4caf50,stroke-width:2px
    classDef data fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef external fill:#fce4ec,stroke:#e91e63,stroke-width:2px
    class UI,User frontend
    class Agent,Memory,Retriever,PromptBuilder,SystemPrompt backend
    class PolicyFiles,Config data
    class LLM external
        </div>
    </div>

    <div class="diagram-container">
        <h2>2. Component Interaction Flow</h2>
        <div class="mermaid">
sequenceDiagram
    participant U as User
    participant F as Frontend<br/>(app.py)
    participant A as Agent<br/>(hr_agent.py)
    participant T as Tools<br/>(tools.py)
    participant P as Policies<br/>(*.txt files)
    participant L as Ollama<br/>(Mistral)
    U->>F: Ask "What is probation period?"
    F->>A: chat(message)
    A->>T: search_policy(query)
    T->>P: Load & search policy files
    P-->>T: Matched policy content
    T-->>A: Policy snippet
    A->>A: Build prompt with:<br/>- System instructions<br/>- Policy context<br/>- Chat history
    A->>L: invoke(prompt)
    L-->>A: Generated response
    A->>A: Store in memory
    A-->>F: Return response
    F-->>U: Display formatted answer
        </div>
    </div>

    <div class="diagram-container">
        <h2>3. Folder Structure & Responsibilities</h2>
        <div class="mermaid">
graph LR
    Root[h:/aiml/agentic/] --> Frontend[frontend/]
    Root --> Backend[backend/]
    Root --> Data[data/]
    Root --> Config[config/]
    Frontend --> App[app.py<br/>üì± Streamlit UI<br/>‚Ä¢ Chat interface<br/>‚Ä¢ User interaction<br/>‚Ä¢ Display messages]
    Backend --> HRAgent[hr_agent.py<br/>ü§ñ Orchestration<br/>‚Ä¢ Manage conversation<br/>‚Ä¢ Coordinate retrieval<br/>‚Ä¢ Call LLM]
    Backend --> Tools[tools.py<br/>üîç Policy Search<br/>‚Ä¢ Load documents<br/>‚Ä¢ Keyword matching<br/>‚Ä¢ Return snippets]
    Backend --> Prompts[prompts.py<br/>üìù Instructions<br/>‚Ä¢ System prompt<br/>‚Ä¢ AI guidelines]
    Data --> Policies[policies/<br/>üìÑ HR Documents<br/>‚Ä¢ leave_policy.txt<br/>‚Ä¢ wfh_policy.txt<br/>‚Ä¢ general_hr_policy.txt]
    Config --> ConfigFile[config.yaml<br/>‚öôÔ∏è Settings<br/>‚Ä¢ Ollama config<br/>‚Ä¢ Agent parameters]
    classDef frontend fill:#e3f2fd,stroke:#2196f3
    classDef backend fill:#f5f5f5,stroke:#4caf50
    classDef data fill:#fff3e0,stroke:#ff9800
    class Frontend,App frontend
    class Backend,HRAgent,Tools,Prompts backend
    class Data,Policies,Config,ConfigFile data
        </div>
    </div>

    <div class="diagram-container">
        <h2>4. Data Flow Architecture</h2>
        <div class="mermaid">
flowchart LR
    subgraph Input
        Q[User Question]
    end
    subgraph "Backend Logic"
        direction TB
        R[Policy Retriever] --> C[Context Builder]
        M[Memory/History] --> C
        SP[System Prompt] --> C
        C --> PR[Complete Prompt]
    end
    subgraph External
        O[Ollama<br/>Mistral LLM]
    end
    subgraph Output
        A[AI Response]
    end
    Q --> R
    Q --> C
    PR --> O
    O --> A
    classDef process fill:#e3f2fd,stroke:#2196f3
    classDef external fill:#fce4ec,stroke:#e91e63
    classDef output fill:#c8e6c9,stroke:#4caf50
    class R,C,M,SP,PR process
    class O external
    class A output
        </div>
    </div>

    <div class="diagram-container">
        <h2>5. LangChain Node Architecture</h2>
        <div class="mermaid">
flowchart TD
    subgraph LangChainCore["LangChain Core Components"]
        InputNode["üì• Input Node<br/>User Message"]
        RetrieverNode["üîç Retriever Node<br/>PolicyRetriever<br/>Keyword Search"]
        PromptNode["üìù Prompt Node<br/>SystemPrompt Template<br/>+ Context Builder"]
        LLMNode["ü§ñ LLM Node<br/>Ollama/Mistral<br/>Temperature: 0.7"]
        OutputNode["üì§ Output Node<br/>Generated Response"]
        MemoryNode["üíæ Memory Node<br/>Conversation Buffer<br/>Max 10 turns"]
    end
    
    subgraph LangChainFlow["Data Flow Through Nodes"]
        InputNode -->|Query Text| RetrieverNode
        InputNode -->|Message| MemoryNode
        MemoryNode -->|Chat History| PromptNode
        RetrieverNode -->|Policy Context| PromptNode
        PromptNode -->|Formatted Prompt| LLMNode
        LLMNode -->|Response Text| OutputNode
        OutputNode -->|Store| MemoryNode
    end
    
    subgraph Integration["Integration Points"]
        FrontendInt["Streamlit<br/>frontend/app.py"]
        BackendInt["HR Agent<br/>backend/hr_agent.py"]
        ToolsInt["Tools<br/>backend/tools.py"]
        PromptsInt["Prompts<br/>backend/prompts.py"]
    end
    
    FrontendInt -->|Calls| BackendInt
    BackendInt -->|Uses| LangChainCore
    ToolsInt -->|Feeds| RetrieverNode
    PromptsInt -->|Feeds| PromptNode
    OutputNode -->|Returns to| FrontendInt
    
    classDef node fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef flow fill:#f5f5f5,stroke:#4caf50,stroke-width:2px
    classDef integration fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    
    class InputNode,RetrieverNode,PromptNode,LLMNode,OutputNode,MemoryNode node
    class LangChainFlow flow
    class FrontendInt,BackendInt,ToolsInt,PromptsInt integration
        </div>
    </div>

    <div class="diagram-container">
        <h2>6. LangChain Node Processing Pipeline</h2>
        <div class="mermaid">
sequenceDiagram
    participant User
    participant Frontend
    participant HRAgent as HR Agent<br/>Node Controller
    participant InputN as Input Node
    participant RetN as Retriever Node
    participant PromptN as Prompt Node
    participant LLMN as LLM Node
    participant MemN as Memory Node
    participant OutputN as Output Node
    
    User->>Frontend: "What is probation period?"
    Frontend->>HRAgent: chat(message)
    HRAgent->>InputN: Process input
    InputN->>RetN: Extract query
    RetN->>RetN: search_policy()
    RetN-->>PromptN: Return context
    HRAgent->>MemN: Get history
    MemN-->>PromptN: Chat history
    PromptN->>PromptN: Build complete prompt
    PromptN->>LLMN: invoke(prompt)
    LLMN->>LLMN: Generate response
    LLMN-->>OutputN: Return text
    OutputN->>OutputN: Format response
    OutputN->>MemN: Store message pair
    MemN-->>HRAgent: Acknowledge
    HRAgent-->>Frontend: Return response
    Frontend-->>User: Display answer
        </div>
    </div>

    <div class="diagram-container">
        <h2>7. LangChain Component Details</h2>
        <div class="mermaid">
graph TB
    subgraph Components["LangChain Components Used"]
        LLMComp["<b>LLM Component</b><br/>---<br/>Type: Ollama<br/>Model: Mistral<br/>Temp: 0.7<br/>Base URL: localhost:11434<br/>Max Tokens: 256"]
        
        ToolComp["<b>Tool/Retriever</b><br/>---<br/>Type: Custom Function<br/>Name: search_hr_policy<br/>Input: Query string<br/>Output: Policy snippet<br/>Max chars: 400"]
        
        PromptComp["<b>Prompt Template</b><br/>---<br/>System Prompt<br/>Context Injection<br/>Chat History<br/>User Query<br/>Combined formatting"]
        
        MemComp["<b>Memory</b><br/>---<br/>Type: Buffer Memory<br/>Max Turns: 10<br/>Storage: List<br/>Format: Role + Content"]
    end
    
    subgraph Workflow["Node Processing Workflow"]
        Step1["1Ô∏è‚É£ User Input<br/>Raw text question"]
        Step2["2Ô∏è‚É£ Context Retrieval<br/>Search policies"]
        Step3["3Ô∏è‚É£ Prompt Assembly<br/>Combine all inputs"]
        Step4["4Ô∏è‚É£ LLM Inference<br/>Generate response"]
        Step5["5Ô∏è‚É£ Memory Storage<br/>Save conversation"]
        Step6["6Ô∏è‚É£ Return Output<br/>Send to frontend"]
        
        Step1 --> Step2
        Step2 --> Step3
        Step3 --> Step4
        Step4 --> Step5
        Step5 --> Step6
    end
    
    LLMComp -.->|Powers| Step4
    ToolComp -.->|Executes| Step2
    PromptComp -.->|Formats| Step3
    MemComp -.->|Manages| Step5
    
    classDef component fill:#e3f2fd,stroke:#2196f3,stroke-width:2px,padding:15px
    classDef step fill:#c8e6c9,stroke:#4caf50,stroke-width:2px
    
    class LLMComp,ToolComp,PromptComp,MemComp component
    class Step1,Step2,Step3,Step4,Step5,Step6 step
        </div>
    </div>

    <div class="diagram-container">
        <h2>8. LangChain Integration in HR Agent</h2>
        <div class="mermaid">
graph LR
    subgraph Backend["Backend Implementation"]
        HRAgent["HRPolicyAgent Class"]
        
        subgraph LangChainUsage["LangChain Usage"]
            LLMLIB["langchain_community.llms<br/>Ollama LLM"]
            TOOLLIB["langchain_core.tools<br/>Tool Definition"]
            MSGLIB["langchain_core.messages<br/>Message Objects"]
        end
        
        subgraph CustomLogic["Custom Logic"]
            PolicyRet["PolicyRetriever<br/>Keyword search"]
            SimpleMem["SimpleMemory<br/>Conversation buffer"]
            Prompts["AGENT_SYSTEM_PROMPT<br/>Instructions"]
        end
    end
    
    HRAgent -->|Imports| LLMLIB
    HRAgent -->|Imports| TOOLLIB
    HRAgent -->|Imports| MSGLIB
    HRAgent -->|Uses| PolicyRet
    HRAgent -->|Uses| SimpleMem
    HRAgent -->|Uses| Prompts
    
    LLMLIB -.->|External| Ollama["üîå Ollama<br/>Mistral Model"]
    
    classDef agent fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef langchain fill:#f5f5f5,stroke:#4caf50,stroke-width:2px
    classDef custom fill:#fff3e0,stroke:#ff9800,stroke-width:2px
    classDef external fill:#fce4ec,stroke:#e91e63,stroke-width:2px
    
    class HRAgent agent
    class LLMLIB,TOOLLIB,MSGLIB langchain
    class PolicyRet,SimpleMem,Prompts custom
    class Ollama external
        </div>
    </div>

    <div class="diagram-container">
        <h2>9. Technology Stack</h2>
        <div class="mermaid">
mindmap
  root((HR Policy<br/>Assistant))
    Frontend
      Streamlit
      HTML/CSS
      Python
    Backend
      LangChain
      Python
      Custom Logic
    LLM
      Ollama
      Mistral
      Local Deployment
    Data
      Text Files
      Policy Documents
      Simple Storage
    Tools
      PolicyRetriever
      Keyword Search
      Memory Buffer
        </div>
    </div>

    <div class="diagram-container">
        <h2>10. Deployment View</h2>
        <div class="mermaid">
flowchart TB
    subgraph "Local Machine"
        subgraph Application
            ST[Streamlit Server<br/>Port 8502]
            BE[Backend Logic<br/>Python Process]
        end
        subgraph Services
            OL[Ollama Service<br/>Port 11434]
            ML[Mistral Model<br/>Local Storage]
        end
        subgraph Data
            PF[Policy Files<br/>data/policies/]
        end
    end
    Browser[Web Browser] -->|HTTP| ST
    ST --> BE
    BE --> OL
    OL --> ML
    BE --> PF
    classDef app fill:#e3f2fd,stroke:#2196f3
    classDef service fill:#fce4ec,stroke:#e91e63
    classDef storage fill:#fff3e0,stroke:#ff9800
    class ST,BE app
    class OL,ML service
    class PF storage
        </div>
    </div>

    <script>
        mermaid.initialize({ 
            startOnLoad: true,
            theme: 'default',
            themeVariables: {
                fontSize: '14px'
            }
        });
    </script>
</body>
</html>
